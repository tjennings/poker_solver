from typing import Dict, List
from collections import defaultdict
from games.base import Game


def best_response_value(
    game: Game,
    strategy: Dict[str, Dict[str, float]],
    exploiting_player: int,
) -> float:
    """
    Compute value of best response for exploiting_player.

    Two-phase algorithm:
    1. Traverse game tree to compute expected value of each action at each info set
    2. Construct best response strategy and evaluate its expected value
    """
    opponent = 1 - exploiting_player

    # Phase 1: Collect conditional expected values for each (info_set, action) pair
    # Maps info_set -> action -> list of (reach_prob, value) pairs
    info_set_values: Dict[str, Dict[str, List[tuple]]] = defaultdict(
        lambda: defaultdict(list)
    )

    def collect(state, opp_reach: float):
        """
        Recursively traverse tree under opponent's strategy.
        For each info set of exploiting player, collect (reach, value) for each action.
        """
        if game.is_terminal(state):
            return game.utility(state, exploiting_player)

        player = game.player(state)
        info_set = game.info_set_key(state)
        actions = game.actions(state)

        if player == exploiting_player:
            # At exploiting player's node, compute value for each action
            for action in actions:
                next_state = game.next_state(state, action)
                action_value = collect(next_state, opp_reach)
                info_set_values[info_set][action].append((opp_reach, action_value))

            # Return placeholder - actual value computed in phase 2
            return 0.0
        else:
            # Opponent plays according to fixed strategy
            opp_strat = strategy.get(info_set, {a: 1/len(actions) for a in actions})
            expected = 0.0
            for action in actions:
                prob = opp_strat.get(action, 0.0)
                if prob > 0:
                    next_state = game.next_state(state, action)
                    value = collect(next_state, opp_reach * prob)
                    expected += prob * value
            return expected

    # Phase 1: Collect all info
    for initial_state in game.initial_states():
        collect(initial_state, opp_reach=1.0)

    # Construct best response: pick action with highest weighted-average value at each info set
    br_strategy: Dict[str, str] = {}
    for info_set, action_data in info_set_values.items():
        best_action = None
        best_value = float('-inf')

        for action, reach_values in action_data.items():
            # Expected value of this action = weighted average by reach probability
            total_reach = sum(r for r, v in reach_values)
            if total_reach > 0:
                avg_value = sum(r * v for r, v in reach_values) / total_reach
            else:
                avg_value = 0.0

            if avg_value > best_value:
                best_value = avg_value
                best_action = action

        br_strategy[info_set] = best_action

    # Phase 2: Evaluate the best response strategy
    def evaluate(state, reach: float) -> float:
        if game.is_terminal(state):
            return reach * game.utility(state, exploiting_player)

        player = game.player(state)
        info_set = game.info_set_key(state)
        actions = game.actions(state)

        if player == exploiting_player:
            # Use best response action
            best_action = br_strategy.get(info_set, actions[0])
            next_state = game.next_state(state, best_action)
            return evaluate(next_state, reach)
        else:
            # Opponent uses fixed strategy
            opp_strat = strategy.get(info_set, {a: 1/len(actions) for a in actions})
            expected = 0.0
            for action in actions:
                prob = opp_strat.get(action, 0.0)
                if prob > 0:
                    next_state = game.next_state(state, action)
                    expected += evaluate(next_state, reach * prob)
            return expected

    # Evaluate over all initial states (card dealings)
    total = 0.0
    initial_states = game.initial_states()
    for state in initial_states:
        total += evaluate(state, reach=1.0)

    return total / len(initial_states)


def compute_exploitability(
    game: Game,
    strategy: Dict[str, Dict[str, float]],
) -> float:
    """
    Compute exploitability of a strategy.

    Exploitability = (BR_value_p0 + BR_value_p1) / 2

    For Nash equilibrium, exploitability = 0.
    """
    br_value_0 = best_response_value(game, strategy, exploiting_player=0)
    br_value_1 = best_response_value(game, strategy, exploiting_player=1)

    return (br_value_0 + br_value_1) / 2
